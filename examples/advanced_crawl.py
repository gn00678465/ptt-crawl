#!/usr/bin/env python3
"""
PTT Stock çˆ¬èŸ²é€²éšä½¿ç”¨ç¯„ä¾‹

æ­¤ç¯„ä¾‹å±•ç¤ºé€²éšåŠŸèƒ½ï¼ŒåŒ…å«ï¼š
- æ‰¹æ¬¡çˆ¬å–å¤šå€‹åˆ†é¡
- å¢é‡çˆ¬å–èˆ‡ç‹€æ…‹ç®¡ç†
- éŒ¯èª¤è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶
- æ•ˆèƒ½ç›£æ§èˆ‡è³‡æºç®¡ç†
- è‡ªè¨‚è³‡æ–™è™•ç†èˆ‡åˆ†æ
"""

import asyncio
import logging
import sys
import time
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Any

import psutil

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(str(Path(__file__).parent.parent))

from src.lib.config_loader import ConfigLoader
from src.lib.logging import setup_logging
from src.lib.redis_client import RedisClient
from src.repositories.article_repository import ArticleRepository
from src.services.crawl_service import CrawlService
from src.services.parser_service import ParserService
from src.services.state_service import StateService


class AdvancedCrawler:
    """é€²éšçˆ¬èŸ²é¡åˆ¥ï¼ŒåŒ…å«è±å¯Œçš„åŠŸèƒ½å’Œç›£æ§"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.start_time = None
        self.start_memory = None
        self.crawl_statistics = defaultdict(int)

    async def initialize_services(self) -> dict[str, Any]:
        """åˆå§‹åŒ–æ‰€æœ‰æœå‹™"""
        self.logger.info("åˆå§‹åŒ–é€²éšçˆ¬èŸ²æœå‹™...")

        # è¼‰å…¥é…ç½®
        config_loader = ConfigLoader()
        config = await config_loader.load_config()

        # åˆå§‹åŒ–æœå‹™
        services = {
            "redis_client": RedisClient(
                url=config.get("REDIS_URL", "redis://localhost:6379"),
                retry_attempts=3,
                retry_delay=1.0,
            ),
            "state_service": None,
            "parser_service": ParserService(),
            "article_repository": ArticleRepository(
                connection_string=config.get(
                    "DATABASE_URL", "postgresql://ptt_user:password@localhost:5432/ptt_crawler"
                )
            ),
            "crawl_service": None,
            "config": config,
        }

        services["state_service"] = StateService(redis_client=services["redis_client"])
        services["crawl_service"] = CrawlService(
            state_service=services["state_service"],
            parser_service=services["parser_service"],
            article_repository=services["article_repository"],
            firecrawl_api_url=config.get("FIRECRAWL_API_URL", "http://localhost:3002"),
            firecrawl_api_key=config.get("FIRECRAWL_API_KEY"),
        )

        return services

    def start_monitoring(self):
        """é–‹å§‹æ•ˆèƒ½ç›£æ§"""
        self.start_time = time.time()
        self.start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        self.logger.info(f"é–‹å§‹ç›£æ§ - åˆå§‹è¨˜æ†¶é«”ä½¿ç”¨: {self.start_memory:.2f} MB")

    def log_current_stats(self):
        """è¨˜éŒ„ç•¶å‰çµ±è¨ˆè³‡è¨Š"""
        current_time = time.time()
        current_memory = psutil.Process().memory_info().rss / 1024 / 1024

        elapsed = current_time - self.start_time
        memory_delta = current_memory - self.start_memory

        self.logger.info(
            f"åŸ·è¡Œæ™‚é–“: {elapsed:.2f}s, è¨˜æ†¶é«”ä½¿ç”¨: {current_memory:.2f}MB ({memory_delta:+.2f}MB)"
        )

    async def batch_crawl_categories(self, services: dict[str, Any]) -> dict[str, Any]:
        """æ‰¹æ¬¡çˆ¬å–å¤šå€‹åˆ†é¡"""
        print("ğŸ¯ æ‰¹æ¬¡çˆ¬å–å¤šå€‹åˆ†é¡")
        print("-" * 30)

        # å®šç¾©è¦çˆ¬å–çš„åˆ†é¡
        categories = ["å¿ƒå¾—", "æ¨™çš„", "è«‹ç›Š", "æ–°è"]
        results = {}

        for category in categories:
            print(f"\nğŸ“ æ­£åœ¨çˆ¬å–åˆ†é¡: {category}")
            self.log_current_stats()

            try:
                result = await services["crawl_service"].crawl_board(
                    board="Stock",
                    category=category,
                    pages=3,  # æ¯å€‹åˆ†é¡çˆ¬ 3 é 
                    incremental=True,  # ä½¿ç”¨å¢é‡çˆ¬å–
                    force=False,
                )

                results[category] = result
                self.crawl_statistics[f"{category}_articles"] = result["articles_crawled"]
                self.crawl_statistics[f"{category}_errors"] = result["errors_count"]

                print(
                    f"   âœ… {category}: {result['articles_crawled']} ç¯‡æ–‡ç« , {result['errors_count']} å€‹éŒ¯èª¤"
                )

                # çŸ­æš«å»¶é²ä»¥é¿å…éåº¦è«‹æ±‚
                await asyncio.sleep(2)

            except Exception as e:
                self.logger.error(f"çˆ¬å–åˆ†é¡ {category} å¤±æ•—: {e}")
                results[category] = {"status": "failed", "error": str(e)}
                print(f"   âŒ {category}: çˆ¬å–å¤±æ•— - {e}")

        return results

    async def incremental_crawl_demo(self, services: dict[str, Any]):
        """å¢é‡çˆ¬å–ç¤ºç¯„"""
        print("\nğŸ”„ å¢é‡çˆ¬å–ç¤ºç¯„")
        print("-" * 20)

        # ç¬¬ä¸€æ¬¡çˆ¬å–
        print("ç¬¬ä¸€æ¬¡çˆ¬å–ï¼ˆå»ºç«‹åŸºç·šï¼‰...")
        result1 = await services["crawl_service"].crawl_board(
            board="Stock",
            category="å¿ƒå¾—",
            pages=2,
            incremental=False,  # ä¸ä½¿ç”¨å¢é‡
            force=True,  # å¼·åˆ¶çˆ¬å–
        )
        print(f"   ç¬¬ä¸€æ¬¡çˆ¬å–: {result1['articles_crawled']} ç¯‡æ–‡ç« ")

        # ç­‰å¾…ä¸€æ®µæ™‚é–“
        print("ç­‰å¾… 5 ç§’å¾Œé€²è¡Œå¢é‡çˆ¬å–...")
        await asyncio.sleep(5)

        # ç¬¬äºŒæ¬¡çˆ¬å–ï¼ˆå¢é‡ï¼‰
        print("ç¬¬äºŒæ¬¡çˆ¬å–ï¼ˆå¢é‡æ¨¡å¼ï¼‰...")
        result2 = await services["crawl_service"].crawl_board(
            board="Stock",
            category="å¿ƒå¾—",
            pages=2,
            incremental=True,  # ä½¿ç”¨å¢é‡
            force=False,
        )
        print(f"   å¢é‡çˆ¬å–: {result2['articles_crawled']} ç¯‡æ–°æ–‡ç« ")

        # æ¯”è¼ƒçµæœ
        print("\nğŸ“Š å¢é‡çˆ¬å–æ•ˆæœ:")
        print(f"   ç¬¬ä¸€æ¬¡çˆ¬å–: {result1['articles_crawled']} ç¯‡")
        print(f"   å¢é‡çˆ¬å–: {result2['articles_crawled']} ç¯‡")
        print(f"   ç¯€çœæ™‚é–“: {result1['duration'] - result2['duration']:.2f} ç§’")

    async def error_handling_demo(self, services: dict[str, Any]):
        """éŒ¯èª¤è™•ç†ç¤ºç¯„"""
        print("\nğŸ›¡ï¸ éŒ¯èª¤è™•ç†ç¤ºç¯„")
        print("-" * 18)

        # æ¸¬è©¦ç¶²è·¯éŒ¯èª¤æ¢å¾©
        print("æ¸¬è©¦æœå‹™é™ç´šï¼ˆRedis å¤±æ•ˆæ™‚é™ç´šåˆ° JSONï¼‰...")

        original_redis_url = services["config"].get("REDIS_URL")

        try:
            # æš«æ™‚ä½¿ç”¨éŒ¯èª¤çš„ Redis URL ä¾†æ¨¡æ“¬æœå‹™ä¸å¯ç”¨
            services["redis_client"]._redis_url = "redis://invalid-host:6379"

            # åŸ·è¡Œçˆ¬å–ï¼Œæ‡‰è©²æœƒè‡ªå‹•é™ç´šåˆ° JSON ç‹€æ…‹ç®¡ç†
            result = await services["crawl_service"].crawl_board(
                board="Stock", category="å¿ƒå¾—", pages=1, incremental=True, force=False
            )

            print(f"   âœ… æœå‹™é™ç´šæˆåŠŸï¼Œçˆ¬å– {result['articles_crawled']} ç¯‡æ–‡ç« ")
            print(f"   âš ï¸ éŒ¯èª¤æ•¸é‡: {result['errors_count']}")

        except Exception as e:
            print(f"   âš ï¸ é æœŸçš„éŒ¯èª¤: {e}")

        finally:
            # æ¢å¾©åŸå§‹ Redis URL
            services["redis_client"]._redis_url = original_redis_url

    async def data_analysis_demo(self, services: dict[str, Any]):
        """è³‡æ–™åˆ†æç¤ºç¯„"""
        print("\nğŸ“ˆ è³‡æ–™åˆ†æç¤ºç¯„")
        print("-" * 16)

        # æŸ¥è©¢æœ€è¿‘çš„æ–‡ç« 
        articles = await services["article_repository"].get_articles_by_board(
            board="Stock", limit=50
        )

        if not articles:
            print("   âš ï¸ æ²’æœ‰æ‰¾åˆ°æ–‡ç« è³‡æ–™")
            return

        # åˆ†ææ–‡ç« åˆ†é¡åˆ†å¸ƒ
        category_count = defaultdict(int)
        author_count = defaultdict(int)
        content_lengths = []

        for article in articles:
            if article.category:
                category_count[article.category] += 1
            author_count[article.author] += 1
            content_lengths.append(len(article.content))

        print(f"ğŸ“Š åˆ†æ {len(articles)} ç¯‡æ–‡ç« :")

        # åˆ†é¡åˆ†å¸ƒ
        print("\n   åˆ†é¡åˆ†å¸ƒ:")
        for category, count in sorted(category_count.items(), key=lambda x: x[1], reverse=True):
            percentage = count / len(articles) * 100
            print(f"     {category}: {count} ç¯‡ ({percentage:.1f}%)")

        # æ´»èºä½œè€…
        print("\n   æœ€æ´»èºä½œè€… (å‰5å):")
        for author, count in sorted(author_count.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"     {author}: {count} ç¯‡")

        # å…§å®¹é•·åº¦çµ±è¨ˆ
        if content_lengths:
            avg_length = sum(content_lengths) / len(content_lengths)
            max_length = max(content_lengths)
            min_length = min(content_lengths)
            print("\n   å…§å®¹é•·åº¦çµ±è¨ˆ:")
            print(f"     å¹³å‡: {avg_length:.0f} å­—")
            print(f"     æœ€é•·: {max_length} å­—")
            print(f"     æœ€çŸ­: {min_length} å­—")

    async def export_analysis_results(self, services: dict[str, Any]):
        """åŒ¯å‡ºåˆ†æçµæœ"""
        print("\nğŸ’¾ åŒ¯å‡ºåˆ†æçµæœ")
        print("-" * 16)

        # æŸ¥è©¢è³‡æ–™
        articles = await services["article_repository"].get_articles_by_board(
            board="Stock", limit=100
        )

        if not articles:
            print("   âš ï¸ æ²’æœ‰è³‡æ–™å¯åŒ¯å‡º")
            return

        # å»ºç«‹åŒ¯å‡ºç›®éŒ„
        output_dir = Path("examples/output/advanced_analysis")
        output_dir.mkdir(parents=True, exist_ok=True)

        # åŒ¯å‡ºè©³ç´° JSON
        import json

        detailed_data = []
        for article in articles:
            detailed_data.append(
                {
                    "id": article.id,
                    "title": article.title,
                    "author": article.author,
                    "category": article.category,
                    "board": article.board,
                    "content_length": len(article.content),
                    "content_summary": article.content[:200] + "..."
                    if len(article.content) > 200
                    else article.content,
                    "publish_date": article.publish_date.isoformat(),
                    "crawl_date": article.crawl_date.isoformat(),
                    "keywords": services["parser_service"].extract_keywords(
                        article.content, max_keywords=5
                    ),
                }
            )

        json_file = (
            output_dir / f"detailed_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(detailed_data, f, ensure_ascii=False, indent=2)

        print(f"   âœ… JSON åŒ¯å‡º: {json_file}")

        # åŒ¯å‡º CSV æ‘˜è¦
        import csv

        csv_file = output_dir / f"summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        with open(csv_file, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["æ¨™é¡Œ", "ä½œè€…", "åˆ†é¡", "å…§å®¹é•·åº¦", "ç™¼å¸ƒæ™‚é–“"])
            for article in articles:
                writer.writerow(
                    [
                        article.title,
                        article.author,
                        article.category or "ç„¡åˆ†é¡",
                        len(article.content),
                        article.publish_date.strftime("%Y-%m-%d %H:%M:%S"),
                    ]
                )

        print(f"   âœ… CSV åŒ¯å‡º: {csv_file}")

    def print_final_statistics(self):
        """é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ"""
        print("\nğŸ“Š åŸ·è¡Œçµ±è¨ˆæ‘˜è¦")
        print("=" * 30)

        total_time = time.time() - self.start_time
        total_memory = psutil.Process().memory_info().rss / 1024 / 1024
        memory_delta = total_memory - self.start_memory

        print(f"ç¸½åŸ·è¡Œæ™‚é–“: {total_time:.2f} ç§’")
        print(f"è¨˜æ†¶é«”ä½¿ç”¨: {total_memory:.2f} MB (è®ŠåŒ–: {memory_delta:+.2f} MB)")

        if self.crawl_statistics:
            print("\nåˆ†é¡çˆ¬å–çµ±è¨ˆ:")
            for key, value in self.crawl_statistics.items():
                print(f"  {key}: {value}")

        total_articles = sum(v for k, v in self.crawl_statistics.items() if k.endswith("_articles"))
        total_errors = sum(v for k, v in self.crawl_statistics.items() if k.endswith("_errors"))

        if total_articles > 0:
            success_rate = (total_articles - total_errors) / total_articles * 100
            articles_per_second = total_articles / total_time
            print("\nç¸½é«”æ•ˆèƒ½:")
            print(f"  çˆ¬å–æ–‡ç« ç¸½æ•¸: {total_articles}")
            print(f"  éŒ¯èª¤ç¸½æ•¸: {total_errors}")
            print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"  çˆ¬å–é€Ÿåº¦: {articles_per_second:.2f} ç¯‡/ç§’")

    async def run_advanced_demo(self):
        """åŸ·è¡Œé€²éšç¤ºç¯„"""
        print("ğŸš€ PTT Stock çˆ¬èŸ²é€²éšåŠŸèƒ½ç¤ºç¯„")
        print("=" * 50)

        services = None
        try:
            # åˆå§‹åŒ–æœå‹™
            self.start_monitoring()
            services = await self.initialize_services()

            print("âœ… æœå‹™åˆå§‹åŒ–å®Œæˆ")

            # åŸ·è¡Œå„é …ç¤ºç¯„
            await self.batch_crawl_categories(services)
            await self.incremental_crawl_demo(services)
            await self.error_handling_demo(services)
            await self.data_analysis_demo(services)
            await self.export_analysis_results(services)

        except Exception as e:
            self.logger.error(f"é€²éšç¤ºç¯„åŸ·è¡Œå¤±æ•—: {e}")
            print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")

        finally:
            # æ¸…ç†è³‡æº
            if services:
                if services["redis_client"]:
                    await services["redis_client"].close()
                if services["article_repository"]:
                    await services["article_repository"].close()

            self.print_final_statistics()
            print("\nğŸ¯ é€²éšåŠŸèƒ½ç¤ºç¯„å®Œæˆ")


async def interactive_menu():
    """äº’å‹•å¼é¸å–®"""
    print("ğŸŒŸ PTT Stock çˆ¬èŸ²é€²éšåŠŸèƒ½é¸å–®")
    print("=" * 40)
    print("1. åŸ·è¡Œå®Œæ•´é€²éšç¤ºç¯„")
    print("2. åƒ…åŸ·è¡Œæ‰¹æ¬¡åˆ†é¡çˆ¬å–")
    print("3. åƒ…åŸ·è¡Œå¢é‡çˆ¬å–ç¤ºç¯„")
    print("4. åƒ…åŸ·è¡Œè³‡æ–™åˆ†æ")
    print("5. æŸ¥çœ‹æ•ˆèƒ½å»ºè­°")
    print("0. é›¢é–‹")

    while True:
        try:
            choice = input("\nè«‹é¸æ“‡åŠŸèƒ½ (0-5): ").strip()

            if choice == "0":
                print("ğŸ‘‹ è¬è¬ä½¿ç”¨!")
                break
            elif choice == "1":
                crawler = AdvancedCrawler()
                await crawler.run_advanced_demo()
                break
            elif choice == "2":
                print("ğŸ¯ åŸ·è¡Œæ‰¹æ¬¡åˆ†é¡çˆ¬å–...")
                crawler = AdvancedCrawler()
                services = await crawler.initialize_services()
                crawler.start_monitoring()
                try:
                    await crawler.batch_crawl_categories(services)
                finally:
                    await services["redis_client"].close()
                    await services["article_repository"].close()
                    crawler.print_final_statistics()
                break
            elif choice == "3":
                print("ğŸ”„ åŸ·è¡Œå¢é‡çˆ¬å–ç¤ºç¯„...")
                crawler = AdvancedCrawler()
                services = await crawler.initialize_services()
                crawler.start_monitoring()
                try:
                    await crawler.incremental_crawl_demo(services)
                finally:
                    await services["redis_client"].close()
                    await services["article_repository"].close()
                    crawler.print_final_statistics()
                break
            elif choice == "4":
                print("ğŸ“ˆ åŸ·è¡Œè³‡æ–™åˆ†æ...")
                crawler = AdvancedCrawler()
                services = await crawler.initialize_services()
                crawler.start_monitoring()
                try:
                    await crawler.data_analysis_demo(services)
                    await crawler.export_analysis_results(services)
                finally:
                    await services["redis_client"].close()
                    await services["article_repository"].close()
                    crawler.print_final_statistics()
                break
            elif choice == "5":
                show_performance_tips()
                continue
            else:
                print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹è¼¸å…¥ 0-5")
                continue

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ä½¿ç”¨è€…ä¸­æ–·ï¼Œç¨‹å¼çµæŸ")
            break
        except Exception as e:
            print(f"âŒ åŸ·è¡ŒéŒ¯èª¤: {e}")
            break


def show_performance_tips():
    """é¡¯ç¤ºæ•ˆèƒ½å»ºè­°"""
    print("\nğŸ’¡ æ•ˆèƒ½æœ€ä½³åŒ–å»ºè­°")
    print("-" * 25)
    print("1. èª¿æ•´çˆ¬å–åƒæ•¸:")
    print("   - request_delay: 1.0-2.0 ç§’ (æ ¹æ“šç¶²è·¯ç‹€æ³)")
    print("   - rate_limit: 30-100 (æ¯åˆ†é˜è«‹æ±‚æ•¸)")
    print("   - concurrent_limit: 2-5 (ä¸¦ç™¼æ•¸)")

    print("\n2. è³‡æ–™åº«æœ€ä½³åŒ–:")
    print("   - å¢åŠ  shared_buffers åˆ° 256MB+")
    print("   - è¨­å®š work_mem åˆ° 16MB+")
    print("   - å•Ÿç”¨æŸ¥è©¢è¨ˆç•«å¿«å–")

    print("\n3. Redis æœ€ä½³åŒ–:")
    print("   - å•Ÿç”¨ AOF æŒä¹…åŒ–")
    print("   - è¨­å®šé©ç•¶çš„ maxmemory")
    print("   - ä½¿ç”¨ allkeys-lru é€å‡ºç­–ç•¥")

    print("\n4. ç³»çµ±è³‡æº:")
    print("   - ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨é‡")
    print("   - ç¢ºä¿è¶³å¤ çš„ç£ç¢Ÿç©ºé–“")
    print("   - ç¶²è·¯é »å¯¬è‡³å°‘ 10Mbps")


if __name__ == "__main__":
    # è¨­å®šæ—¥èªŒ
    setup_logging(level=logging.INFO)

    print("ğŸš€ æ­¡è¿ä½¿ç”¨ PTT Stock çˆ¬èŸ²é€²éšåŠŸèƒ½!")
    print("   æœ¬ç¤ºç¯„å±•ç¤ºæ‰¹æ¬¡çˆ¬å–ã€å¢é‡çˆ¬å–ã€éŒ¯èª¤è™•ç†ç­‰é€²éšåŠŸèƒ½")
    print("   è«‹ç¢ºä¿ç³»çµ±å·²æ­£ç¢ºé…ç½®ä¸¦æœ‰è¶³å¤ è³‡æº\n")

    try:
        asyncio.run(interactive_menu())
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ä½¿ç”¨è€…ä¸­æ–·ï¼Œç¨‹å¼çµæŸ")
    except Exception as e:
        print(f"\nâŒ ç¨‹å¼éŒ¯èª¤: {e}")
